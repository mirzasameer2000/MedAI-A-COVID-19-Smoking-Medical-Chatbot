# -*- coding: utf-8 -*-
"""Class_Practice_AIAgent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16RHt-pd0rty8h5DOg4VV7nUXEW7RmIY8
"""

!pip install llama-index llama-index-embeddings-huggingface llama-index-llms-huggingface bitsandbytes

import os
import kagglehub
import pandas as pd

from tqdm import tqdm

from llama_index.core import Settings
from llama_index.core import Document
from llama_index.core import StorageContext
from llama_index.core import VectorStoreIndex
from llama_index.core import load_index_from_storage
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

from matplotlib import pyplot as plt

# Download latest version
path = kagglehub.dataset_download(handle="googleai/dataset-metadata-for-cord19")

print("Path to dataset files:", path)

os.listdir(path)

filename_with_path = path + "/" + os.listdir(path)[0]
filename_with_path

df_meta_cord19 = pd.read_csv(filename_with_path)
df_meta_cord19.head()

df_meta_cord19.info()

df_meta_cord19_filtered = df_meta_cord19[df_meta_cord19['description'].notnull()]
df_meta_cord19_filtered.info()

df_meta_cord19_filtered = df_meta_cord19_filtered[['description']]
#df_meta_cord19_filtered["description"] = df_meta_cord19_filtered["description"].apply(lambda x: x[0])
df_meta_cord19_filtered["word_count"] = df_meta_cord19_filtered["description"].apply(lambda x: len(str(x).split(" ")))
df_meta_cord19_filtered.head()

df_meta_cord19_filtered["word_count"].describe()

model_name = "sentence-transformers/all-MiniLM-L6-v2"

Settings.llm = None
Settings.embed_model = HuggingFaceEmbedding(model_name=model_name, device="cuda")

model_name = "sentence-transformers/all-MiniLM-L6-v2"

Settings.llm = None
Settings.embed_model = HuggingFaceEmbedding(model_name=model_name, device="cuda")

chunks = []
chunk_size_by_words = 150

for text in tqdm(df_meta_cord19_filtered["description"].values):
    # Check if the text is a string before applying split()
    if isinstance(text, str):
        text_split = text.split(" ")
        for i in range(0, len(text_split), chunk_size_by_words):
            chunk = " ".join(text_split[i:i + chunk_size_by_words])
            chunks.append(Document(text=chunk))
    # If not a string (e.g., nan), skip or handle appropriately
    else:
        # Option 1: Skip the non-string value
        continue
        # Option 2: Replace nan with an empty string
        # chunks.append(Document(text=""))

len(chunks)

index = VectorStoreIndex.from_documents(chunks, show_progress=True, insert_batch_size=len(chunks))

persist_dir="storage"

index.storage_context.persist(persist_dir=persist_dir)
print(f"VectorStoreIndex saved to {persist_dir}.")

loaded_storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
index = load_index_from_storage(loaded_storage_context)
print(f"VectorStoreIndex loaded from {persist_dir}.")

query_text = "What are the similar effects on patients who smoke and those with COVID-19 in the human body??"
query_engine = index.as_query_engine(similarity_top_k=10)

response = query_engine.query(query_text)

# Print the results
print(f"Query: {query_text}\n")
print("Source details:")
for node in response.source_nodes:
  text = node.text.replace('\n', " ")
  print(f"Node ID: {node.node_id}\nScore: {node.score}\n{text[:200]}\n")

!pip install llama-index-llms-huggingface --quiet

from llama_index.llms.huggingface import HuggingFaceLLM

llm = HuggingFaceLLM(
    model_name="colesmcintosh/Llama-3.2-1B-Instruct-Mango",
    tokenizer_name="colesmcintosh/Llama-3.2-1B-Instruct-Mango",
    context_window=2048,
    max_new_tokens=256,
    device_map="cuda:0",
    generate_kwargs={"temperature": 0.95, "do_sample": True},
)

Settings.llm = llm

from llama_index.core.memory import ChatMemoryBuffer

chat_engine = index.as_chat_engine(

    chat_mode="context",

    memory=ChatMemoryBuffer.from_defaults(token_limit=32000),

    system_prompt=(
        "You are a medical chatbot, able to have normal interactions."
    )
)

while True:
  query = input("> ")                                     # Bekérjük a felhasználótól a kérdést.
  if query.lower() == "quit":                             # Ha a felhasználó azt írja, hogy "quit", akkor a ciklus megszakad.
      break

  # Streamelt válasz
  print("Agent: ", end="", flush=True)                    # Kiírjuk, hogy az ágens válaszolni fog.
  response = chat_engine.stream_chat(query)               # Ez a sor elküldi a kérdést a chat motornak, és megkapjuk a választ.
  for token in response.response_gen:                     # Ez a ciklus kiírja a választ részleteiben (tokenekként) streamelve.
      print(token, end="", flush=True)
  print()                                                 # Egy új sort írunk ki a válasz után.

chat_engine.reset()